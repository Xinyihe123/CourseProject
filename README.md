# CS410 CourseProject -- Reproduce a paper

# Documentation:

Team LARA

Datasets from http://times.cs.uiuc.edu/~wang296/Data/

References: 

Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505

Hongning Wang, Yue Lu and Chengxiang Zhai. Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach. The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2010), p783-792, 2010.

The codes in LRR are downloaded from Internet. These are references. Source: Hongning Wang, Yue Lu and Chengxiang Zhai. Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach. The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2010), p783-792, 2010.




## Presentation: https://mediaspace.illinois.edu/media/t/1_fo2gtfej

## files:

clean.py: <br>
data preprocess: First we remove the reviews with any missing aspect rating or document length less than 50 words (to keep the content coverage of all possible aspects).  Then we  convert all the words into lower cases and remove punctuations and stop words.  In vocab.txt we write vocabulary appearance based on reviews. If a word appears in several times in the same review, it would only be counted as once.  We then filtered out words that have less than ten occurences.  <br>

load.py: <br>
build matrix for reviews and generate results. Load data for testing. <br>

lara.py: <br>
The LARA model, mainly the aspect modeling part, using EM algorithm.In this program, we implemented function such as update_mu, update_beta, E_step, M_step etc. Gererated the alpha and s, which are the review-level k dimensional( 7 for our data) aspect weight vector and rating vector. The overall rating for the review can be drawn from the Gaussian distribution with mean alpah.T dot product s, and variance delta. <br>

Data: The test data we use, download from http://times.cs.uiuc.edu/~wang296/Data/: TripAdvisor Data Set: JSON <br>

Results: <br>

High rating words collections and the aspect rating weight for each reviews represented as matrix. <br>


LRR <br>:
Downloaded from Internet. Source: Hongning Wang, Yue Lu and Chengxiang Zhai. Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach. The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2010), p783-792, 2010.  <br>
The LRR model implemented by Hongning Wang using Java <br>

We used some codes in LRR, since the LARA we built is a generative model of LRR. We used some parameter initilizing codes in LRR. 

## Required packages in Python3:
numpy <br>
scipy <br>
nltk

## Run:
python3 lara.py <br>

## Project Members:
Xinyi He <br>
Weijiang Li <br>
Dingsen Shi  <br>
Qunyu Shen <br>
Ziyuan Wei <br>

We decided to work with another team to build this model since the challenge we were facing when trying to understand the methods and alogrithm are extremely hard. <br> 

Implementation of Model:<br>


The inputs of this model for each review d are: (epsilon, gamma, beta, mu, sigma, delta), with hidden parameters: sigma_inverse, alpha, sigma_square, eta, phi. <br>
Epsilon, gamma, beta, mu, sigma, delta, phi, alpha, eta and the rating vector r are used in calculate log-likelihood. <br>
The outputs are alpha and s, which are the review-level k dimensional( 7 for our data) aspect weight vector and rating vector, used for gererate the final overall ratings of review d. <br>
The detail of this model can be found in the paper. <br>


This model involves more than ten parameters, some of them are generated by mutivariate Gaussian distribution, variational distribution, Dirichlet distribution, and multinomial distribution, and they are updated using gradient based method, which are hard to implement and transfer the complex math equations into codes. <br>
We spend most of our time on reading and understanding the paper and the math methods in the paper. Thus although we spent more than 20*5 hours on this project, we can only produce a simple and crude model and test one dataset in the paper. <br>

### Team contribution:
Since each steps and parameters in the EM algorithm are closely related to each other, we usually coded and debuged together through zoom, all of our teammates contributed their 100% effort on this project.
